{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "import bettertimeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Design your own Neural Net\n",
    "\n",
    "## ~~Ray Hettinger~~\n",
    "\n",
    "## Varun Nayyar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This is\n",
    "\n",
    "- A fun mix of ML and Software\n",
    "- A deeper dive into Neural Nets than pytorch\n",
    "- Mostly iterative design and analysis\n",
    "- A lot of live coding (that I'm going to regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Why? \n",
    "\n",
    "- Neural Nets are easy\n",
    "- Everyone loves Neural Nets\n",
    "- Good way to illustrate good software mixed with ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This is not\n",
    "\n",
    "- A good way to implement a Neural Net Library in 2019\n",
    "- Building computational graphs\n",
    "- Automatic Differentiation (autograd) or how to do it\n",
    "- GPU programming\n",
    "- See @chewxy (if he ever returns) for the above\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Nets\n",
    "\n",
    "![nn.png](resources/nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward \n",
    "\n",
    "- Fully Connected Layer\n",
    "    - $y=Wx + b$\n",
    "    - This is just a matrix multiplication\n",
    "- Forward\n",
    "    - $y = tanh(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Backprop was invented independently 3 times.\n",
    "- Was thought to be useless for a long time - Hiton spent many years on approximate methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def backward(self, dldy):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aside\n",
    "\n",
    "- $Wx+b$ has these shapes\n",
    "    - x is (Indim,)\n",
    "    - W is (Outdim, Indim)\n",
    "    - b is (Outdim,)\n",
    "- We want to batch our x, we don't want to do this\n",
    "- How do we initialize the W and b?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(...):\n",
    "    y = []\n",
    "    for vector in x:\n",
    "        y.append(W @ x + b)\n",
    "    return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnected(Layer):\n",
    "    def __init__(self, indim, hiddendim):\n",
    "        self.W = np.ones((hiddendim, indim))\n",
    "        self.b = np.zeros(hiddendim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.W @ x + self.b\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's quickly test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-53319591cd33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullyConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-8d9553167b44>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 10)"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "x = np.random.randn(N, 10)\n",
    "l = FullyConnected(10, 32)\n",
    "l.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hmm\n",
    "\n",
    "- What should the shapes of $x$ and $W$ be?\n",
    "    - if $x$ is (N, I) then W should be (I, O)\n",
    "        - `x @ W`\n",
    "    - if $x$ is (I, N) then W should be (O, I)\n",
    "        - `W @ x`\n",
    "- is there a compute consideration here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt1: 10000 loops, best of 10: 110 usec per loop\n",
      "opt2: 10000 loops, best of 10: 123 usec per loop\n"
     ]
    }
   ],
   "source": [
    "def forward():\n",
    "    import numpy as np\n",
    "    N = 3000\n",
    "    indim = 20\n",
    "    hiddendim = 40\n",
    "\n",
    "    w = np.random.randn(indim, hiddendim)\n",
    "    x = np.random.randn(N, indim)\n",
    "\n",
    "    def timeit_opt1():\n",
    "        x @ w\n",
    "\n",
    "    wT = w.T\n",
    "    # wopp = np.random.randn(hiddendim, indim)\n",
    "    xT = x.T\n",
    "    # xopp = np.random.randn(indim, N)\n",
    "\n",
    "    def timeit_opt2():\n",
    "        wT @ xT\n",
    "\n",
    "bettertimeit.bettertimeit(forward, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design\n",
    "\n",
    "- N >> I (usually)\n",
    "- $x$ being I, N is a column major format\n",
    "    - natural mathematical form\n",
    "    - If this was fortran, matlab or julia \n",
    "    - (why col major and these languages are found in mathy applications a lot)\n",
    "- Python and C are row major = N, I\n",
    "    - faster \n",
    "    - more natural to have N the leading index anyway\n",
    "    - Coda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(Layer):\n",
    "    def __init__(self, indim, hiddendim):\n",
    "        self.W = np.ones((indim, hiddendim))\n",
    "        self.b = np.zeros(hiddendim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x @ self.W + self.b\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 100\n",
    "x = np.random.randn(N, 10)\n",
    "l = FullyConnected(10, 32)\n",
    "y = l.forward(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation Design\n",
    "\n",
    "- zeros is a bad idea (very slow backprop - will come back to)\n",
    "- Many different approaches (Xavier, He, etc)\n",
    "    - Xavier is random normal(0, scale) where scale is 2/I+O\n",
    "    \n",
    "- Classmethod or init arg?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My opinion\n",
    "\n",
    "- Classmethod\n",
    "    - choices would show up in methods (less reliance on doc)\n",
    "    - user needs to know all the options \n",
    "    - many methods would be very similar - code duplication\n",
    "    - init would either be user unfriendly (takes W and b) or have a bad defaut\n",
    "- init arg\n",
    "    - classmethod's are best when we have very different arguments\n",
    "    - lot of code can be shared\n",
    "    - Maybe even allow it to be a function?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(Layer):\n",
    "    def __init__(self, indim, hiddendim, init=\"xavier\"):\n",
    "        if init == \"xavier\":\n",
    "            scale = np.sqrt(2/(indim+hiddendim))\n",
    "        elif init == \"he\":\n",
    "            scale = np.sqrt(2/indim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown initialiser: {init}\")\n",
    "        self.W = np.random.randn(indim, hiddendim) * scale\n",
    "        self.b = np.zeros(hiddendim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x @ self.W + self.b\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "\n",
    "N = 100\n",
    "x = np.random.randn(N, 10)\n",
    "l = FullyConnected(10, 32)\n",
    "y = l.forward(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Equations\n",
    "\n",
    "\n",
    "- Fully Connected\n",
    "    - $\\frac{dy}{dx} = W^T$\n",
    "    - $\\frac{dy}{dW} = x^T$\n",
    "    - $\\frac{dy}{db} = 1$\n",
    "    - Chain Rule + Matrix math\n",
    "        - $dL/dy$ is same shape as y - (N,O)\n",
    "        - $\\frac{dL}{dx} = \\frac{dL}{dy} W^T$\n",
    "        - $\\frac{dL}{dW} = x^T\\frac{dL}{dy}$\n",
    "        - $\\frac{dL}{db} = \\frac{dL}{dy}$\n",
    "- Tanh\n",
    "    - $\\frac{dy}{dx} = 1-tanh^2(x)$\n",
    "    - $\\frac{dL}{dx} = (1-tanh^2(x)) * \\frac{dL}{dy}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tanh(Layer):\n",
    "    \n",
    "    def backward(self, dldy):\n",
    "        dldx = (1 - (np.tanh(x)) ** 2) * dldy\n",
    "        return dldx\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    \n",
    "    def backward(self, dldy):\n",
    "        dldw = dldy @ self.W.T\n",
    "        dldb = dldy\n",
    "        dldx = x.T @ dldy\n",
    "        return dldx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait\n",
    "\n",
    "- We don't have access to the input, $x$ in the backward pass!\n",
    "- How should we solve this?\n",
    "    - Cache it on the forward pass?\n",
    "    - Save it to a \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
