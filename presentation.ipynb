{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "import bettertimeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Design your own Neural Net\n",
    "\n",
    "## ~~Ray Hettinger~~\n",
    "\n",
    "## Varun Nayyar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## What\n",
    "\n",
    "- We're going to write a simple neural net codebase\n",
    "- We only want a fully connected layer aproach with custom optimisation\n",
    "- We aren't building ResNet, but we want our design to be flexible, efficient and memory aware\n",
    "- We also want to make it easy to use!\n",
    "\n",
    "## Why? \n",
    "\n",
    "- Neural Nets are easy\n",
    "- Everyone loves Neural Nets\n",
    "- Good way to illustrate good software mixed with ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This is\n",
    "\n",
    "- A fun mix of ML and Software\n",
    "- A deeper dive into Neural Nets than pytorch\n",
    "- Mostly iterative design and analysis\n",
    "- A lot of live coding (that I'm going to regret)\n",
    "- Gratuitous classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This is not\n",
    "\n",
    "- A good way to implement a Neural Net Library in 2019\n",
    "- Building computational graphs\n",
    "- Automatic Differentiation (autograd) or how to do it\n",
    "- GPU programming\n",
    "- See @chewxy for the above\n",
    "- Using non standardlib (i.e. no `attrs` et al)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Nets\n",
    "\n",
    "![nn.png](resources/nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Backprop was invented independently 3 times.\n",
    "- Was thought to be useless for a long time - Hiton spent many years on approximate methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward \n",
    "\n",
    "- Fully Connected Layer\n",
    "    - $y=Wx + b$\n",
    "    - This is just a matrix multiplication\n",
    "- TanH\n",
    "    - $y = tanh(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aside\n",
    "\n",
    "- $Wx+b$ has these shapes\n",
    "    - x is (Indim,)\n",
    "    - W is (Outdim, Indim)\n",
    "    - b is (Outdim,)\n",
    "- How do we initialize the W and b?\n",
    "- We want to batch our x, we don't want to do this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(*args):\n",
    "    y = []\n",
    "    for vector in x:\n",
    "        y.append(W @ x + b)\n",
    "    return y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's try this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnected(Layer):\n",
    "    def __init__(self, indim, hiddendim):\n",
    "        self.W = np.ones((hiddendim, indim))\n",
    "        self.b = np.zeros(hiddendim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.W @ x + self.b\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's quickly test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-53319591cd33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullyConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-8d9553167b44>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 100 is different from 10)"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "x = np.random.randn(N, 10)\n",
    "l = FullyConnected(10, 32)\n",
    "l.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hmm\n",
    "\n",
    "- What should the shapes of $x$ and $W$ be?\n",
    "    - if $x$ is (I, N) then W should be (O, I)\n",
    "        - `W @ x`\n",
    "    - if $x$ is (N, I) then W should be (I, O)\n",
    "        - `x @ W`\n",
    "- is there a compute consideration here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt1: 10000 loops, best of 10: 110 usec per loop\n",
      "opt2: 10000 loops, best of 10: 120 usec per loop\n"
     ]
    }
   ],
   "source": [
    "def forward():\n",
    "    import numpy as np\n",
    "    N = 3000\n",
    "    indim = 20\n",
    "    hiddendim = 40\n",
    "\n",
    "    w = np.random.randn(indim, hiddendim)\n",
    "    x = np.random.randn(N, indim)\n",
    "\n",
    "    def timeit_opt1():\n",
    "        x @ w\n",
    "\n",
    "    wT = w.T\n",
    "    xT = x.T\n",
    "\n",
    "    def timeit_opt2():\n",
    "        wT @ xT\n",
    "\n",
    "bettertimeit.bettertimeit(forward, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Design\n",
    "\n",
    "- N >> I (usually)\n",
    "- $x$ (I, N) is column major!\n",
    "    - natural mathematical form in code!\n",
    "    - If this was fortran, matlab or julia \n",
    "- Python and C are row major = N, I\n",
    "    - more efficient access in these languages\n",
    "    - N as leading index matches most python/C conventions\n",
    "    - Code no longer matches the mathematics :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnected(Layer):\n",
    "    def __init__(self, indim, hiddendim):\n",
    "        self.W = np.ones((indim, hiddendim))\n",
    "        self.b = np.zeros(hiddendim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x @ self.W + self.b\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 100\n",
    "x = np.random.randn(N, 10)\n",
    "l = FullyConnected(10, 32)\n",
    "y = l.forward(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initialisation Design\n",
    "\n",
    "- zeros is a bad idea (very slow backprop)\n",
    "- Many different approaches (Xavier, He, etc)\n",
    "    - Xavier is random normal(0, scale) where scale is 2/I+O\n",
    "    - He et al is random normal (0, scale) where scale is 2/I\n",
    "    \n",
    "- Design: Classmethod or init arg?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## My Opinion\n",
    "\n",
    "- Classmethod\n",
    "    - choices would show up in methods (less reliance on doc)\n",
    "    - user needs to know all the options \n",
    "    - many methods would be very similar - code duplication\n",
    "    - init would either be user unfriendly (takes W and b) or have a bad defauLt\n",
    "- Init arg\n",
    "    - classmethod's are best when we have very different arguments\n",
    "    - lot of code can be shared\n",
    "    - Maybe even allow it to be a function?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class FullyConnected(Layer):\n",
    "    def __init__(self, indim, hiddendim, init=\"xavier\"):\n",
    "        if init == \"xavier\":\n",
    "            scale = np.sqrt(2/(indim+hiddendim))\n",
    "        elif init == \"he\":\n",
    "            scale = np.sqrt(2/indim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown initialiser: {init}\")\n",
    "        self.W = np.random.randn(indim, hiddendim) * scale\n",
    "        self.b = np.zeros(hiddendim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x @ self.W + self.b\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test\n",
    "\n",
    "N = 100\n",
    "x = np.random.randn(N, 10)\n",
    "l = FullyConnected(10, 32)\n",
    "y = l.forward(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Equations\n",
    "\n",
    "\n",
    "- Fully Connected\n",
    "    - $\\frac{dy}{dx} = W^T$\n",
    "    - $\\frac{dy}{dW} = x^T$\n",
    "    - $\\frac{dy}{db} = 1$\n",
    "    - Chain Rule + Matrix math\n",
    "        - $dL/dy$ is same shape as y - (N,O)\n",
    "        - $\\frac{dL}{dx} = \\frac{dL}{dy} W^T$\n",
    "        - $\\frac{dL}{dW} = x^T\\frac{dL}{dy}$\n",
    "        - $\\frac{dL}{db} = \\frac{dL}{dy}$\n",
    "- Tanh\n",
    "    - $\\frac{dy}{dx} = 1-tanh^2(x)$\n",
    "    - $\\frac{dL}{dx} = (1-tanh^2(x)) * \\frac{dL}{dy}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def backward(self, dldy):\n",
    "        pass\n",
    "\n",
    "class Tanh(Layer):\n",
    "    \n",
    "    def backward(self, dldy):\n",
    "        dldx = (1 - (np.tanh(x)) ** 2) * dldy\n",
    "        return dldx\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    \n",
    "    def backward(self, dldy):\n",
    "        dldw = dldy @ self.W.T\n",
    "        dldb = dldy\n",
    "        dldx = x.T @ dldy\n",
    "        # TODO: param updates\n",
    "        return dldx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wait\n",
    "\n",
    "- We don't have access to the input, $x$ in the backward pass!\n",
    "- How should we solve this?\n",
    "    - Cache it on the forward pass?\n",
    "    - Or expect it as an argument to the backward function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## My Opinion\n",
    "\n",
    "- Caching\n",
    "    - Easiest for user - just call backward and it just works\n",
    "    - Odd side effects - call forward twice and gradient will only be on second forward\n",
    "    - Uneccessary work if we're doing inference only\n",
    "- Argument\n",
    "    - Explicit is better than implicit!\n",
    "    - Easier to test - we can separate the functionality.\n",
    "    - Allows for possible optimisations or memory control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "\n",
    "    def backward(self, dldy, x):\n",
    "        y = self.forward(x)\n",
    "        return (1 - y**2) * dldy\n",
    "    \n",
    "\n",
    "class FullyConnected(Layer):\n",
    "\n",
    "    def backward(self, dldy, x):\n",
    "        dldw = x.T @ dldy\n",
    "        dldb = np.sum(dldy, axis=0)\n",
    "        dldx = dldy @ self.W.T\n",
    "        \n",
    "        # TODO: param updates\n",
    "        \n",
    "        return dldx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideration\n",
    "\n",
    "- the backward pass of the tanh can be expressed in terms of the output\n",
    "- the output of the activation layer is usually the input of the fully connected layer\n",
    "- passing the input in isn't always the best option!\n",
    "- ReLU may prefer a different cache value!\n",
    "- If batch is large, each layer will now store a copy of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Forward pass returns the value it wants on the backward pass?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = np.tanh(x)\n",
    "        return y, y\n",
    "\n",
    "    def backward(self, dldy, y):\n",
    "        return (1 - y**2) * dldy\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = x @ self.W + self.b\n",
    "        return y, x\n",
    "\n",
    "    def backward(self, dldy, x):\n",
    "        dldw = x.T @ dldy\n",
    "        dldb = np.sum(dldy, axis=0)\n",
    "        dldx = dldy @ self.W.T\n",
    "        \n",
    "        # TODO: param updates\n",
    "        \n",
    "        return dldx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally\n",
    "\n",
    "- Pass both input and output to backwards function\n",
    "- Allow layer to do custom caching if need be, but give control of object to top level\n",
    "\n",
    "We clearly need a container object here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Network:\n",
    "    def __init__(self, *layers):\n",
    "        self.network = tuple(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cacheList = []\n",
    "        for l in self.network:\n",
    "            x, c = l.forward(x)\n",
    "            cacheList.append(c)\n",
    "        return x, cacheList\n",
    "\n",
    "    def backward(self, dldx, cachelist):\n",
    "        for l, c in zip(reversed(self.network), reversed(cachelist)):\n",
    "            dldx = l.backward(dldx, c)\n",
    "        return dldx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def backward(self, dldy, cache):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "    def __init__(self, indim, hiddendim):\n",
    "        super().__init__()\n",
    "        self.W = np.random.randn(indim, hiddendim) * np.sqrt(2 / indim)\n",
    "        self.b = np.zeros(hiddendim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x @ self.W + self.b\n",
    "        return y, x\n",
    "\n",
    "    def backward(self, dldy, x):\n",
    "        dldw = x.T @ dldy\n",
    "        dldb = np.sum(dldy, axis=0)\n",
    "        dldx = dldy @ self.W.T\n",
    "\n",
    "        return dldx\n",
    "\n",
    "\n",
    "class Tanh(Layer):\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = np.tanh(x)\n",
    "        return y, y\n",
    "\n",
    "    def backward(self, dldy, y):\n",
    "        return (1 - y**2) * dldy\n",
    "\n",
    "\n",
    "class Network(Layer):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.network = tuple(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cacheList = []\n",
    "        for l in self.network:\n",
    "            x, c = l.forward(x)\n",
    "            cacheList.append(c)\n",
    "        return x, cacheList\n",
    "\n",
    "    def backward(self, dldx, cachelist):\n",
    "        for l, c in zip(reversed(self.network), reversed(cachelist)):\n",
    "            dldx = l.backward(dldx, c)\n",
    "        return dldx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = np.random.randn(100, 10)\n",
    "y = np.random.randn(100, 3)\n",
    "dldy = np.random.randn(100, 3)\n",
    "net = Network(\n",
    "    FullyConnected(10, 20),\n",
    "    Tanh(),\n",
    "    FullyConnected(20, 3),\n",
    "    Tanh()\n",
    ")\n",
    "\n",
    "yhat, ca = net.forward(x)\n",
    "dldx = net.backward(dldy, ca)\n",
    "dldx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisers!\n",
    "\n",
    "- SGD\n",
    "    - $\\theta^{new} = \\theta^{curr} - \\eta \\nabla L$\n",
    "    - stateless\n",
    "    - $\\eta$ is the learning rate\n",
    "- Momentum Gradient Descent\n",
    "    - $\\nu^{new} = \\alpha \\nu^{curr}  + \\eta \\nabla L$\n",
    "    - $\\theta^{new} = \\theta^{curr} - \\nu^{new}$\n",
    "    - not stateless\n",
    "    - $\\alpha$ is the momentum param\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "\n",
    "class FullyConnected(Layer):\n",
    "\n",
    "    def backward(self, dldy, x):\n",
    "        dldw = x.T @ dldy\n",
    "        dldb = np.sum(dldy, axis=0)\n",
    "        dldx = dldy @ self.W.T\n",
    "\n",
    "        self.W -= dldw * LR\n",
    "        self.b -= dldb * LR\n",
    "\n",
    "        return dldx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Design\n",
    "\n",
    "- Should train be a method of Network or a function\n",
    "\n",
    "## My opinion\n",
    "\n",
    "- Network actually matches the signature of a Layer!\n",
    "- We can compose a network of networks! \n",
    "- Train methods are very variable, we should provide flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    \"\"\"Eg loss function\"\"\"\n",
    "    def loss(self, y, yhat):\n",
    "        return np.mean((y - yhat)**2 / 2)\n",
    "\n",
    "    def loss_gradient(self, y, yhat):\n",
    "        return np.expand_dims(np.mean(yhat - y, axis=-1), axis=-1)\n",
    "    \n",
    "\n",
    "def train(network, data, numepochs):\n",
    "    mse = MSELoss()\n",
    "    for i in range(numepochs):\n",
    "        x, y = data\n",
    "        yhat, cachelist = network.forward(x)\n",
    "        dldy = mse.loss_gradient(y, yhat)\n",
    "        network.backward(dldy, cachelist)\n",
    "        print(f\"Epoch {i}, loss: {mse.loss(y, yhat)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
